import Foundation
//import CoreML

protocol ChatGPTClient {
    func generateResponse(for message: String) async throws -> String
    func clearHistory()
}

class ChatManager {
    private let preprocessor: MessagePreprocessor
    private let cache: ResponseCache
    private let apiClient: ChatGPTClient
    private let localAI: LocalAIHandler
    
    init(preprocessor: MessagePreprocessor, apiClient: ChatGPTClient) {
        self.preprocessor = preprocessor
        self.cache = ResponseCache.shared
        self.apiClient = apiClient
        self.localAI = LocalAIHandler()
        
        // Print debug info on initialization
        print("=== ML System Configuration ===")
        MLDeviceCapabilities.debugComputeInfo()
        print("============================")
    }
    
    func processMessage(_ message: String) async throws -> String {
        // Handle ML commands first
        if message.lowercased().hasPrefix("!ml") {
            return handleMLCommand(message.lowercased())
        }
        
        let useLocalAI = UserDefaults.standard.bool(forKey: "useLocalAI")
        
        // Local AI mode - all processing happens locally
        if useLocalAI {
            await MainActor.run {
                print("ðŸ§  Using LocalAI for query: \(message)")
            }
            let response = try await localAI.generateResponse(for: message)
            await MainActor.run {
                print("ðŸ§  LocalAI generated response")
            }
            return response + "\n[Using LocalAI]"
        }
        
        // Regular processing flow when LocalAI is disabled
        do {
            // Try cache first
            if preprocessor.shouldCache(message) && !preprocessor.isMLRelatedQuery(message) {
                await MainActor.run {
                    print("ðŸ” Checking cache for: \(message)")
                }
                if let cachedResponse = try cache.findSimilarResponse(for: message) {
                    await MainActor.run {
                        print("âœ¨ Cache hit! Using cached response")
                    }
                    return cachedResponse + "\n[Retrieved using BERT]"
                }
                await MainActor.run {
                    print("ðŸ’« No cache hit, generating new response")
                }
            }
            
            // Try API
            let response = try await apiClient.generateResponse(for: message)
            
            // Cache if appropriate
            if preprocessor.shouldCache(message) && !preprocessor.isMLRelatedQuery(message) {
                try cache.cacheResponse(query: message, response: response)
                await MainActor.run {
                    print("ðŸ“¥ Cached new response")
                }
            }
            
            return response
            
        } catch let error as NSError {
            // If network is offline, fallback to local AI
            if error.domain == NSURLErrorDomain && error.code == NSURLErrorNotConnectedToInternet {
                await MainActor.run {
                    print("ðŸŒ Network offline, falling back to Local AI")
                }
                let response = try await localAI.generateResponse(for: message)
                return response + "\n[Using LocalAI - Network Offline]"
            }
            throw error
        }
    }
    
    private func handleMLCommand(_ command: String) -> String {
        // Clean up command by:
        // 1. Trimming whitespace and newlines
        // 2. Converting to lowercase
        // 3. Removing extra spaces
        let cleanCommand = command
            .trimmingCharacters(in: .whitespacesAndNewlines)
            .lowercased()
            .components(separatedBy: .whitespacesAndNewlines)
            .filter { !$0.isEmpty }
            .joined(separator: " ")
        
        switch cleanCommand {
        case "!ml", "!ml help":  // Handle both !ml and !ml help the same way
            return """
            ðŸ¤– ANIE ML Commands:
            
            !ml status  - Show ML system status including:
            â€¢ CoreML/ANE status
            â€¢ BERT model status
            â€¢ Cache statistics
            
            !ml clear   - Clear all caches:
            â€¢ BERT response cache
            â€¢ Conversation history
            â€¢ Persisted data
            
            !ml help    - Show this help message
            
            Note: BERT caching is automatically disabled for programming questions.
            Current similarity threshold: \(String(format: "%.2f", cache.threshold))
            """
            
        case "!ml clear":
            // Clear BERT cache
            cache.clearCache()
            // Clear conversation history
            apiClient.clearHistory()
            // Clear UserDefaults
            UserDefaults.standard.synchronize()
            
            return """
            âœ¨ All caches cleared:
            â€¢ BERT response cache
            â€¢ Conversation history
            â€¢ Persisted data
            """
            
        case "!ml status":
            return """
            === ML System Status ===
            
            ðŸ§  CoreML Status:
            â€¢ ANE Available: \(MLDeviceCapabilities.hasANE)
            â€¢ Compute Units: \(MLDeviceCapabilities.getOptimalComputeUnits())
            
            ðŸ¤– BERT Status:
            â€¢ Model Active: \(EmbeddingsService.shared.generator != nil)
            â€¢ Dimension: \(EmbeddingsService.shared.generator?.modelInfo()["embeddingDimension"] ?? 0)
            â€¢ Cache Operations: \(EmbeddingsService.shared.usageCount)
            â€¢ Using ANE: \(MLDeviceCapabilities.hasANE)
            
            ðŸ’¾ Cache Status:
            â€¢ Similarity Threshold: \(String(format: "%.2f", cache.threshold))
            â€¢ Cached Items: \(cache.getCacheSize())
            
            ========================
            """
            
        case "!ml cache":
            return """
            === Cache System Status ===
            
            ðŸ“Š Cache Configuration:
            â€¢ Total Cached Items: \(cache.getCacheSize())
            â€¢ Similarity Threshold: \(String(format: "%.2f", cache.threshold))
            â€¢ Storage Type: Persistent (UserDefaults)
            
            ðŸ¤– BERT Integration:
            â€¢ Model Status: \(EmbeddingsService.shared.generator != nil ? "Active" : "Inactive")
            â€¢ Total Operations: \(EmbeddingsService.shared.usageCount)
            â€¢ Vector Dimension: \(EmbeddingsService.shared.generator?.modelInfo()["embeddingDimension"] ?? 0)
            
            âš™ï¸ Cache Behavior:
            â€¢ Skips ML/AI Queries: Yes
            â€¢ Skips Programming Queries: Yes
            â€¢ Uses Semantic Search: Yes
            â€¢ Word Overlap Required: 70%
            
            ========================
            """
            
        default:
            return "Unknown ML command. Use !ml help to see available commands."
        }
    }
    
    // Add a method to test ML performance
    func runMLPerformanceTest() async {
        print("\n=== ML Performance Test ===")
        let testMessages = [
            "Hello, how are you?",
            "What's the weather like today?",
            "Can you help me with a complex programming task?",
            "Tell me a joke",
            "Explain quantum computing"
        ]
        
        print("Testing preprocessing and embedding generation...")
        for message in testMessages {
            do {
                let start = CFAbsoluteTimeGetCurrent()
                let shouldProcess = try preprocessor.shouldProcessMessage(message)
                let end = CFAbsoluteTimeGetCurrent()
                print("Message: '\(message.prefix(20))...'")
                print("Should process: \(shouldProcess)")
                print("Processing time: \((end - start) * 1000)ms\n")
            } catch {
                print("Error processing message: \(error)")
            }
        }
        print("========================")
    }
}

// Add to MLDeviceCapabilities
extension MLDeviceCapabilities {
    static func getSystemInfo() -> [String: Any] {
        return [
            "hasANE": hasANE,
            "computeUnits": getOptimalComputeUnits(),
            "modelActive": EmbeddingsService.shared.generator != nil,
            "usageCount": EmbeddingsService.shared.usageCount
        ]
    }
} 
